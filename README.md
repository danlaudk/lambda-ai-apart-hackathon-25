# getting started with vLLM on lambda.ai via ssh

This repository contains multiple AI/LLM related projects and utilities.

## Project Structure

### vllm/
vLLM Model Manager API - A Flask-based API service for managing vLLM instances and runtime model switching. Supports multiple models including Qwen, Llama, Mistral, and Phi-4.

**Key Features:**
- Runtime model switching via HTTP API
- Multi-model support
- Remote access capabilities
- Local and remote setup guides

See `vllm/README.md` for detailed documentation.

### ai_assessment_dora/
.csv data file and pyscript

## Getting Started

### Prerequisites
- Python 3.10+
- Required Python packages (see `vllm/requirements.txt`)

### Quick Start
For vLLM setup, see `vllm/README.md` or `vllm/QUICKSTART.md` . 

can then run Local_setup
## Notes


